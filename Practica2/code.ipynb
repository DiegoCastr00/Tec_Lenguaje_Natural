{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Daphne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Daphne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Daphne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Daphne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Daphne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package floresta to\n",
      "[nltk_data]     C:\\Users\\Daphne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package floresta is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('floresta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(file_name, encoding='utf-8'):\n",
    "    with open(file_name, 'r', encoding=encoding) as file:\n",
    "        filedata = file.readlines()\n",
    "        article = filedata[0].split(\". \")\n",
    "        sentences = []\n",
    "        # removing special characters and extra whitespaces\n",
    "        for sentence in article:\n",
    "            sentence = re.sub('[^a-zA-Z\\s]', '', sentence)\n",
    "            sentence = re.sub('\\s+', ' ', sentence)\n",
    "            sentences.append(sentence.strip())\n",
    "        sentences.pop()\n",
    "        display = \" \".join(sentences)\n",
    "        print('Initial Text:')\n",
    "        print(display)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_words(sent):\n",
    "    cnt = 0\n",
    "    words = word_tokenize(sent)\n",
    "    for word in words:\n",
    "        cnt = cnt +1\n",
    "    return cnt\n",
    "\n",
    "def cnt_in_sent(sentences):\n",
    "    txt_data = []\n",
    "    i = 0\n",
    "    for sent in sentences: \n",
    "        i = i + 1\n",
    "        cnt = cnt_words(sent)\n",
    "        temp = {'id' : i, 'word_cnt' : cnt}\n",
    "        txt_data.append(temp)\n",
    "    return txt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(sentences):\n",
    "    i=0\n",
    "    freq_list = []\n",
    "    for sent in sentences:\n",
    "        i = i + 1\n",
    "        freq_dict = {} \n",
    "        words = word_tokenize(sent, language='spanish')\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in freq_dict:\n",
    "                freq_dict[word] += 1\n",
    "            else:\n",
    "                freq_dict[word] = 1\n",
    "            temp = {'doc_id' : i, 'freq_dict' : freq_dict}\n",
    "        freq_list.append(temp)\n",
    "    return freq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TF(text_data, freq_list):\n",
    "    tf_scores = []\n",
    "    for item in freq_list:\n",
    "       ID = item['id']\n",
    "       for k in item['freq_dict']:\n",
    "            temp = {\n",
    "                'id': item['id'],\n",
    "                'tf_score': item['freq_dict'][k]/text_data[ID-1]['word_cnt'],'key': k\n",
    "            }\n",
    "            tf_scores.append(temp)\n",
    "    return tf_scores\n",
    "\n",
    "#calculating the inverse document frequency\n",
    "def calc_IDF(text_data, freq_list):\n",
    "    idf_scores = []\n",
    "    cnt = 0\n",
    "    for item in freq_list:\n",
    "        cnt = cnt +1\n",
    "        for k in item['freq_dict']:\n",
    "            val = sum([k in it['freq_dict'] for it in freq_list])\n",
    "            temp = {\n",
    "                'id':cnt,\n",
    "                'idf_score': math.log(len(text_data)/val + 1),\n",
    "                'key': k\n",
    "            }\n",
    "            idf_scores.append(temp)\n",
    "    return idf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_TFIDF(tf_scores, idf_scores):\n",
    "    tfidf_scores = []\n",
    "    for j in idf_scores:\n",
    "        for i in tf_scores:\n",
    "            if j['key'] == i['key'] and j['id'] == i['id']:\n",
    "                temp = {\n",
    "                    'id': j['id'],\n",
    "                    'tfidf_scores' : j['idf_score'] * i['tf_score'],  # Corrección realizada aquí\n",
    "                    'key': j['key']\n",
    "                }\n",
    "                tfidf_scores.append(temp)\n",
    "    return tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_scores(tfidf_scores, sentences, text_data):\n",
    "    sent_data = []\n",
    "    for txt in text_data:\n",
    "        score = 0\n",
    "        for i in range(0, len(tfidf_scores)):\n",
    "            t_dict = tfidf_scores[i]\n",
    "            if txt['id'] == t_dict['id']:\n",
    "                score = score + t_dict['tfidf_scores']\n",
    "        temp = {\n",
    "            'id': txt['id'],  # Reemplazar 'j' con 'txt' aquí\n",
    "            'score': score,\n",
    "            'sentence': sentences[txt['id']-1]\n",
    "        }\n",
    "        sent_data.append(temp)\n",
    "    return sent_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(sent_data):\n",
    "    cnt = 0\n",
    "    summary =[]\n",
    "    for t_dict in sent_data:\n",
    "        cnt = cnt + t_dict['score']\n",
    "    avg = cnt/  len(sent_data)\n",
    "    for sent in sent_data:\n",
    "        if sent['score'] >= (avg * 0.9) :\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = \". \".join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Text:\n",
      "En la actual sociedad cambiante el sector de la distribucin se enfrenta a numerosos desafos y presiones competitivas asociadas con la lucha por mantener unos mrgenes ajustados\n",
      "Final Summary: \n",
      "En la actual sociedad cambiante el sector de la distribucin se enfrenta a numerosos desafos y presiones competitivas asociadas con la lucha por mantener unos mrgenes ajustados\n"
     ]
    }
   ],
   "source": [
    "sentences = clean_text('PRUEBA1.txt')\n",
    "text_data = cnt_in_sent(sentences)\n",
    "\n",
    "freq_list = freq_dict(sentences)\n",
    "tf_scores = calc_TF(text_data, freq_list)\n",
    "idf_scores = calc_IDF(text_data, freq_list)\n",
    "\n",
    "tfidf_scores = calc_TFIDF(tf_scores, idf_scores)\n",
    "\n",
    "sent_data = sent_scores(tfidf_scores, sentences, text_data)\n",
    "result = summary(sent_data)\n",
    "print('Final Summary: ')\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "primer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
