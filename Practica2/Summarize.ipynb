{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import operator\n",
    "import statistics\n",
    "stop_words = set(stopwords.words('spanish') + list(punctuation))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def getFile(fname):\n",
    "    try:\n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()  \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "def tokenizador(texto):\n",
    "    oraciones= ['. ','\\n']\n",
    "    tokens = []\n",
    "\n",
    "    token_actual = ''\n",
    "    for caracter in texto:\n",
    "        if caracter in oraciones:\n",
    "            if token_actual:\n",
    "                tokens.append(token_actual)\n",
    "                token_actual = ''\n",
    "        else:\n",
    "            token_actual += caracter\n",
    "\n",
    "    if token_actual:\n",
    "        tokens.append(token_actual)\n",
    "\n",
    "    return tokens\n",
    "        \n",
    "def clean(textoTokenizado):\n",
    "    \n",
    "    #textoTokenizado = tokenizador(txt)\n",
    "    newTokens = []\n",
    "    for token in textoTokenizado:\n",
    "        special_chars = \"!@#$%^&*()-_=+[]{}|;:'\\\",.<>/?`~\"\n",
    "        \n",
    "        # Replace special characters with ' '\n",
    "        stripped = ''.join(char if char.isalnum() or char.isspace() or char in special_chars else ' ' for char in token)\n",
    "\n",
    "        # Change any whitespace to one space\n",
    "        stripped = ' '.join(stripped.split())\n",
    "\n",
    "        # Remove start and end white spaces\n",
    "        stripped = stripped.strip()\n",
    "        \n",
    "        newTokens.append(stripped)\n",
    "    return newTokens\n",
    "\n",
    "#unir conteo palabras,, freq palabras y tf oracion \n",
    "\n",
    "def countWordSentence(sentences):\n",
    "    # Se crea un diccionario guardando el numero de palabras por oracion\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in sentences:\n",
    "        i += 1 \n",
    "        words = sent.split()\n",
    "        count = len(words)\n",
    "        temp = {'sentence_id' : i, 'num_words' : count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info\n",
    "\n",
    "def create_freq_dict(sents):\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word not in stop_words:\n",
    "                if word in freq_dict:\n",
    "                    freq_dict[word] += 1\n",
    "                else:\n",
    "                    freq_dict[word] = 1\n",
    "                temp = {'sentence_id' : i, 'wordsfreq': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list\n",
    "\n",
    "\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "    TF_scores = []\n",
    "    \n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['sentence_id']\n",
    "        for k in tempDict['wordsfreq']:\n",
    "            temp = {'sentence_id' : id,\n",
    "                    'TF_score' : tempDict['wordsfreq'][k]/doc_info[id-1]['num_words'],\n",
    "                     'word': k}\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores\n",
    "\n",
    "\n",
    "import math \n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    idf = ln(total number of docs/number of docs with term in it)\n",
    "    \"\"\"\n",
    "    \n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['wordsfreq'].keys():\n",
    "            count = sum([k in tempDict['wordsfreq'] for tempDict in freqDict_list])\n",
    "            temp = {'sentence_id' : counter, 'IDF_score' : math.log(len(doc_info)/count), 'word' : k}\n",
    "    \n",
    "            IDF_scores.append(temp)\n",
    "                \n",
    "    return IDF_scores\n",
    "\n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    \"\"\"\n",
    "    TFIDF is computed by multiplying the coressponding\n",
    "    TF and IDF values of each term. \n",
    "    \"\"\"\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['word'] == i['word'] and j['sentence_id'] == i['sentence_id']:\n",
    "                temp = {'sentence_id' : i['sentence_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                       'word' : i['word']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores\n",
    "\n",
    "#se une hasta aqui\n",
    "\n",
    "def global_frequency(txtClean):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary with the frequency \n",
    "    count of every word in the text\n",
    "    \"\"\"\n",
    "    freq_table = {}\n",
    "    text = ' '.join(txtClean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "    return freq_table\n",
    "\n",
    "\n",
    "def get_keywords(txtClean, n):\n",
    "\n",
    "    freq_table = global_frequency(txtClean)\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = True) \n",
    "    keywords = []\n",
    "    for i in range(0, n):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords\n",
    "\n",
    "def weigh_keywords(txtClean, TFIDF_scores, n):\n",
    "\n",
    "    keywords = get_keywords(txtClean, n)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['word'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores\n",
    "\n",
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "    \n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['sentence_id'] == temp_dict['sentence_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'sentence_id' : doc['sentence_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['sentence_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "\n",
    "    return sentence_info\n",
    "\n",
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sum of scores\n",
    "        of all the sentences.\n",
    "        \"\"\"\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sentence scores \n",
    "        and stores them in an array.\n",
    "        \"\"\"\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop is for getting the sumamry by \n",
    "        extracting sentences by an if clause\n",
    "        \"\"\"\n",
    "        if(sent['sent_score']) >= avg: # + 1.5*stdev:\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumirF(texto):\n",
    "    texto = sent_tokenize(texto)\n",
    "    txtClean = clean(texto)\n",
    "    txtDic = countWordSentence(txtClean)\n",
    "    freqDict = create_freq_dict(txtClean)\n",
    "    TF = computeTF(txtDic,freqDict)\n",
    "    IDF = computeIDF(txtDic,freqDict)\n",
    "    TFIDF_scores = computeTFIDF(TF, IDF)\n",
    "\n",
    "    TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 5)\n",
    "        \n",
    "    sentence_info = get_sent_score(TFIDF_scores, txtClean, txtDic)\n",
    "    summary = get_summary(sentence_info)\n",
    "    print(summary)\n",
    "    texto = summary\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inteligencia artificial general (AGI)\n",
      "Esta categoría -Artificial General Intelligence- se alcanza cuando una máquina adquiere capacidades cognitivas a nivel humano.\n",
      "Es decir, cuando puede realizar cualquier tarea intelectual que realiza una persona.\n",
      "El Congreso de EE.UU., por su parte, convocó este martes al CEO de OpenAI, Sam Altman, a responder preguntas sobre ChatGPT.\n",
      "Carlos Ignacio Gutiérrez, investigador de políticas públicas en el Future of Life Institute, explicó a BBC Mundo que uno de los grandes desafíos que presenta la IA es que \"no existe un cuerpo colegiado de expertos que deciden cómo regularlo, como ocurre, por ejemplo, con el Panel Intergubernamental sobre Cambio Climático (IPCC)\".\n",
      "\"¿Deberíamos desarrollar mentes no humanas que eventualmente podrían superarnos en número, ser más inteligentes, hacernos obsoletos y reemplazarnos?\n",
      "\", cuestionaron.\n",
      "El futuro de la IA: hacia inteligencias artificiales realmente inteligentes\n",
      "Ramón López de Mántaras\n",
      "Instituto de Investigación en Inteligencia Artificial (IIIA), Bellaterra, España\n",
      "Este capítulo contiene algunas reflexiones sobre inteligencia artificial (IA).\n",
      "Se describen brevemente los principales modelos, insistiendo en la importancia de la corporalidad como aspecto clave para conseguir una IA de naturaleza general.\n",
      "A continuación se aborda la necesidad de proporcionar a las máquinas conocimientos de sentido común que hagan posible avanzar hacia el ambicioso objetivo de construir IA de tipo general.\n",
      "También se comentan las últimas tendencias en IA basadas en el análisis de grandes cantidades de datos que han hecho posibles progresos espectaculares en épocas muy recientes, con una alusión a las dificultades presentes hoy en los enfoques de la IA.\n",
      "Por ejemplo, en el siglo XVII, Descartes se preguntó si un complejo sistema mecánico compuesto de engranajes, poleas y tubos podría, en principio, emular el pensamiento.\n",
      "Dos siglos después, la metáfora fueron los sistemas telefónicos ya que parecía que sus conexiones se podían asimilar a una red neuronal.\n",
      "LA HIPÓTESIS DEL SISTEMA DE SÍMBOLOS FÍSICOS: IA DÉBIL VERSUS IA FUERTE\n",
      "En una ponencia, con motivo de la recepción del prestigioso Premio Turing en 1975, Allen Newell y Herbert Simon (Newell y Simon, 1975) formularon la hipótesis del Sistema de Símbolos Físicos según la cual «todo sistema de símbolos físicos posee los medios necesarios y suficientes para llevar a cabo acciones inteligentes».\n",
      "Por otra parte, dado que los seres humanos somos capaces de mostrar conductas inteligentes en el sentido general, entonces, de acuerdo con la hipótesis, nosotros somos también sistemas de símbolos físicos.\n",
      "Conviene aclarar a que se refieren Newell y Simon cuando hablan de Sistema de Símbolos Físicos (SSF).\n",
      "Un SSF consiste en un conjunto de entidades denominadas símbolos que, mediante relaciones, pueden ser combinados formando estructuras más grandes —como los átomos que se combinan formando moléculas— y que pueden ser transformados aplicando un conjunto de procesos.\n",
      "Estos procesos pueden generar nuevos símbolos, crear y modificar relaciones entre símbolos, almacenar símbolos, comparar si dos símbolos son iguales o distintos, etcétera.\n",
      "En definitiva, de acuerdo con la hipótesis SSF, la naturaleza del sustrato (circuitos electrónicos o redes neuronales) carece de importancia siempre y cuando dicho sustrato permita procesar símbolos.\n",
      "En cualquier caso, su validez o refutación se deberá verificar de acuerdo con el método científico, con ensayos experimentales.\n",
      "La IA es precisamente el campo científico dedicado a intentar verificar esta hipótesis en el contexto de los ordenadores digitales, es decir, verificar si un ordenador convenientemente programado es capaz o no de tener conducta inteligente de tipo general.\n",
      "Exhibir inteligencia específica es otra cosa bien distinta.\n",
      "Aunque estrictamente la hipótesis SSF se formuló en 1975, ya estaba implícita en las ideas de los pioneros de la IA en los años cincuenta e incluso en las ideas de Alan Turing en sus escritos pioneros (Turing, 1948, 1950) sobre máquinas inteligentes.\n",
      "La IA débil, por otro lado, consistiría, según Searle, en construir programas que realicen tareas específicas y, obviamente sin necesidad de tener estados mentales.\n",
      "También se asocia con la IA débil el hecho de formular y probar hipótesis acerca de aspectos relacionados con la mente (por ejemplo la capacidad de razonar deductivamente, de aprender inductivamente, etcétera) mediante la construcción de programas que llevan a cabo dichas funciones aunque sea mediante procesos completamente distintos a los que lleva a cabo el cerebro.\n",
      "LOS PRINCIPALES MODELOS EN IA: SIMBÓLICO, CONEXIONISTA, EVOLUTIVO Y CORPÓREO\n",
      "El modelo dominante en IA ha sido el simbólico, que tiene sus raíces en la hipótesis SSF.\n",
      "De hecho, sigue siendo muy importante y actualmente se considera el modelo clásico en IA (también denominado por el acrónimo GOFAI, de Good Old Fashioned AI).\n",
      "Es decir, la IA simbólica opera con representaciones abstractas del mundo real que se modelan mediante lenguajes de representación basados principalmente en la lógica matemática y sus extensiones.\n",
      "Por este motivo, los primeros sistemas inteligentes resolvían principalmente problemas que no requieren interactuar directamente con el entorno como, por ejemplo, demostrar sencillos teoremas matemáticos o jugar al ajedrez —los programas que juegan al ajedrez no necesitan de hecho la percepción visual para ver las piezas en el tablero ni actuadores para mover las piezas—.\n",
      "Inteligencia artificial general (AGI) Esta categoría -Artificial General Intelligence- se alcanza cuando una máquina adquiere capacidades cognitivas a nivel humano.\n",
      "Es decir, cuando puede realizar cualquier tarea intelectual que realiza una persona.\n",
      "El Congreso de EE.UU., por su parte, convocó este martes al CEO de OpenAI, Sam Altman, a responder preguntas sobre ChatGPT.\n",
      "Carlos Ignacio Gutiérrez, investigador de políticas públicas en el Future of Life Institute, explicó a BBC Mundo que uno de los grandes desafíos que presenta la IA es que \"no existe un cuerpo colegiado de expertos que deciden cómo regularlo, como ocurre, por ejemplo, con el Panel Intergubernamental sobre Cambio Climático (IPCC)\".\n",
      "\" Deberíamos desarrollar mentes no humanas que eventualmente podrían superarnos en número, ser más inteligentes, hacernos obsoletos y reemplazarnos?\n",
      "\", cuestionaron.\n",
      "El futuro de la IA: hacia inteligencias artificiales realmente inteligentes Ramón López de Mántaras Instituto de Investigación en Inteligencia Artificial (IIIA), Bellaterra, España Este capítulo contiene algunas reflexiones sobre inteligencia artificial (IA).\n",
      "Por ejemplo, en el siglo XVII, Descartes se preguntó si un complejo sistema mecánico compuesto de engranajes, poleas y tubos podría, en principio, emular el pensamiento.\n",
      "Un SSF consiste en un conjunto de entidades denominadas símbolos que, mediante relaciones, pueden ser combinados formando estructuras más grandes como los átomos que se combinan formando moléculas y que pueden ser transformados aplicando un conjunto de procesos.\n",
      "Estos procesos pueden generar nuevos símbolos, crear y modificar relaciones entre símbolos, almacenar símbolos, comparar si dos símbolos son iguales o distintos, etcétera.\n",
      "En definitiva, de acuerdo con la hipótesis SSF, la naturaleza del sustrato (circuitos electrónicos o redes neuronales) carece de importancia siempre y cuando dicho sustrato permita procesar símbolos.\n",
      "Exhibir inteligencia específica es otra cosa bien distinta.\n"
     ]
    }
   ],
   "source": [
    "def resumir(file):\n",
    "    texto = getFile(file)\n",
    "    texto = sent_tokenize(texto)\n",
    "\n",
    "    txtClean = clean(texto)\n",
    "    txtDic = countWordSentence(txtClean)\n",
    "    freqDict = create_freq_dict(txtClean)\n",
    "    TF = computeTF(txtDic,freqDict)\n",
    "    IDF = computeIDF(txtDic,freqDict)\n",
    "    TFIDF_scores = computeTFIDF(TF, IDF)\n",
    "\n",
    "    TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 5)\n",
    "        \n",
    "    sentence_info = get_sent_score(TFIDF_scores, texto, txtDic)\n",
    "\n",
    "    summary = get_summary(sentence_info)\n",
    "    #print(summary)\n",
    "    texto = summary\n",
    "    return texto\n",
    "\n",
    "resumenG = []\n",
    "resumen1 = resumir('prueba2_ia.txt')\n",
    "resumenG.append(resumen1) \n",
    "resumen2 = resumir('texto.txt')\n",
    "resumenG.append(resumen2)\n",
    "resumen_total = '\\n'.join(resumenG)\n",
    "print(resumen_total)\n",
    "resumenF = resumirF(resumen_total)\n",
    "#print(resumenF)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
