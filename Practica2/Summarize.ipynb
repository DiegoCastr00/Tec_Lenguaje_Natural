{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import operator\n",
    "import statistics\n",
    "import math \n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('spanish') + list(punctuation))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def getFile(fname):\n",
    "    try:\n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "def clean2(textoTokenizado):\n",
    "    newTokens = []\n",
    "    for token in textoTokenizado:\n",
    "        pattern = r\"[^a-zA-Z0-9 \\n.,()áéóíúÁÉÍÓÚ]\"\n",
    "\n",
    "        stripped = re.sub(pattern, \"\", token)\n",
    "\n",
    "        # Change any whitespace to one space\n",
    "        stripped = ' '.join(stripped.split())\n",
    "\n",
    "        # Remove start and end white spaces\n",
    "        stripped = stripped.strip()\n",
    "        \n",
    "        newTokens.append(stripped)\n",
    "    return newTokens\n",
    "\n",
    "def clean(textoTokenizado):\n",
    "    newTokens = []\n",
    "    for token in textoTokenizado:\n",
    "        pattern = r\"[^a-zA-Z0-9 \\n]\"\n",
    "\n",
    "        stripped = re.sub(pattern, \"\", token)\n",
    "\n",
    "        # Change any whitespace to one space\n",
    "        stripped = ' '.join(stripped.split())\n",
    "\n",
    "        # Remove start and end white spaces\n",
    "        stripped = stripped.strip()\n",
    "        \n",
    "        newTokens.append(stripped)\n",
    "    return newTokens\n",
    "\n",
    "def global_frequency(txtClean):\n",
    "\n",
    "    freq_table = {}\n",
    "    text = ' '.join(txtClean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "    return freq_table\n",
    "\n",
    "\n",
    "def get_keywords(txtClean, n):\n",
    "\n",
    "    freq_table = global_frequency(txtClean)\n",
    "    #sort in descending order\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = False) \n",
    "    #print(freq_table_sorted)\n",
    "    keywords = []\n",
    "    for i in range(0, n):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords\n",
    "\n",
    "def weigh_keywords(txtClean, TFIDF_scores, n):\n",
    "\n",
    "    keywords = get_keywords(txtClean, n)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['word'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores\n",
    "\n",
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "\n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['sentence_id'] == temp_dict['sentence_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'sentence_id' : doc['sentence_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['sentence_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "    return sentence_info\n",
    "\n",
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "\n",
    "        if(sent['sent_score']) >= avg: # + 1.5*stdev:\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(sentences):\n",
    "    TFIDF_scores = []\n",
    "    num_words_per_sentence = []\n",
    "    doc_info = []\n",
    "    datos = []\n",
    "\n",
    "    for i, sent in enumerate(sentences, start=1):\n",
    "        words = sent.split()\n",
    "        count = len(words)\n",
    "        temp = {'sentence_id': i, 'num_words': count, 'wordsfreq': {}}\n",
    "\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word not in stop_words:\n",
    "                if word in temp['wordsfreq']:\n",
    "                    temp['wordsfreq'][word] += 1\n",
    "                else:\n",
    "                    temp['wordsfreq'][word] = 1\n",
    "\n",
    "        for word, freq in temp['wordsfreq'].items():\n",
    "            temp['wordsfreq'][word] = freq / count\n",
    "        temp2 = {'sentece_id':i, 'word':word}\n",
    "        doc_info.append(temp)\n",
    "        num_words_per_sentence.append({'sentence_id': i, 'num_words': count})\n",
    "\n",
    "    IDF_scores = []\n",
    "    for word in set(word for doc in doc_info for word in doc['wordsfreq']):\n",
    "        count = sum(1 for doc in doc_info if word in doc['wordsfreq'])\n",
    "        idf_score = math.log(len(doc_info) / count)\n",
    "        IDF_scores.append({'word': word, 'IDF_score': idf_score})\n",
    "\n",
    "    # Calcular la puntuación TF-IDF para cada palabra en cada oración y almacenar en TFIDF_scores\n",
    "    for doc in doc_info:\n",
    "        for word, tf_score in doc['wordsfreq'].items():\n",
    "            idf_score = next(item['IDF_score'] for item in IDF_scores if item['word'] == word)\n",
    "            tfidf_score = tf_score * idf_score\n",
    "            TFIDF_scores.append({'sentence_id': doc['sentence_id'], 'word': word, 'TFIDF_score': tfidf_score})\n",
    "        \n",
    "\n",
    "    return TFIDF_scores, num_words_per_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El futuro de la IA hacia inteligencias artificiales realmente inteligentes Ramón López de Mántaras Instituto de Investigación en Inteligencia Artificial (IIIA), Bellaterra, Espaa Este capítulo contiene algunas reflexiones sobre inteligencia artificial (IA).\n",
      "Se describen brevemente los principales modelos, insistiendo en la importancia de la corporalidad como aspecto clave para conseguir una IA de naturaleza general.\n",
      "A continuación se aborda la necesidad de proporcionar a las máquinas conocimientos de sentido común que hagan posible avanzar hacia el ambicioso objetivo de construir IA de tipo general.\n",
      "También se comentan las últimas tendencias en IA basadas en el análisis de grandes cantidades de datos que han hecho posibles progresos espectaculares en épocas muy recientes, con una alusión a las dificultades presentes hoy en los enfoques de la IA.\n",
      "Por ejemplo, en el siglo XVII, Descartes se preguntó si un complejo sistema mecánico compuesto de engranajes, poleas y tubos podría, en principio, emular el pensamiento.\n",
      "Dos siglos después, la metáfora fueron los sistemas telefónicos ya que parecía que sus conexiones se podían asimilar a una red neuronal.\n",
      "LA HIPÓTESIS DEL SISTEMA DE SÍMBOLOS FÍSICOS IA DÉBIL VERSUS IA FUERTE En una ponencia, con motivo de la recepción del prestigioso Premio Turing en 1975, Allen Newell y Herbert Simon (Newell y Simon, 1975) formularon la hipótesis del Sistema de Símbolos Físicos según la cual todo sistema de símbolos físicos posee los medios necesarios y suficientes para llevar a cabo acciones inteligentes.\n",
      "Por otra parte, dado que los seres humanos somos capaces de mostrar conductas inteligentes en el sentido general, entonces, de acuerdo con la hipótesis, nosotros somos también sistemas de símbolos físicos.\n",
      "Un SSF consiste en un conjunto de entidades denominadas símbolos que, mediante relaciones, pueden ser combinados formando estructuras más grandes como los átomos que se combinan formando moléculas y que pueden ser transformados aplicando un conjunto de procesos.\n",
      "Estos procesos pueden generar nuevos símbolos, crear y modificar relaciones entre símbolos, almacenar símbolos, comparar si dos símbolos son iguales o distintos, etcétera.\n",
      "En definitiva, de acuerdo con la hipótesis SSF, la naturaleza del sustrato (circuitos electrónicos o redes neuronales) carece de importancia siempre y cuando dicho sustrato permita procesar símbolos.\n",
      "En cualquier caso, su validez o refutación se deberá verificar de acuerdo con el método científico, con ensayos experimentales.\n",
      "Exhibir inteligencia específica es otra cosa bien distinta.\n",
      "Aunque estrictamente la hipótesis SSF se formuló en 1975, ya estaba implícita en las ideas de los pioneros de la IA en los aos cincuenta e incluso en las ideas de Alan Turing en sus escritos pioneros (Turing, 1948, 1950) sobre máquinas inteligentes.\n",
      "La IA débil, por otro lado, consistiría, según Searle, en construir programas que realicen tareas específicas y, obviamente sin necesidad de tener estados mentales.\n",
      "También se asocia con la IA débil el hecho de formular y probar hipótesis acerca de aspectos relacionados con la mente (por ejemplo la capacidad de razonar deductivamente, de aprender inductivamente, etcétera) mediante la construcción de programas que llevan a cabo dichas funciones aunque sea mediante procesos completamente distintos a los que lleva a cabo el cerebro.\n",
      "LOS PRINCIPALES MODELOS EN IA SIMBÓLICO, CONEXIONISTA, EVOLUTIVO Y CORPÓREO El modelo dominante en IA ha sido el simbólico, que tiene sus raíces en la hipótesis SSF.\n",
      "De hecho, sigue siendo muy importante y actualmente se considera el modelo clásico en IA (también denominado por el acrónimo GOFAI, de Good Old Fashioned AI).\n",
      "Es decir, la IA simbólica opera con representaciones abstractas del mundo real que se modelan mediante lenguajes de representación basados principalmente en la lógica matemática y sus extensiones.\n",
      "Por este motivo, los primeros sistemas inteligentes resolvían principalmente problemas que no requieren interactuar directamente con el entorno como, por ejemplo, demostrar sencillos teoremas matemáticos o jugar al ajedrez los programas que juegan al ajedrez no necesitan de hecho la percepción visual para ver las piezas en el tablero ni actuadores para mover las piezas.\n"
     ]
    }
   ],
   "source": [
    "#ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def resumir(file, n):\n",
    "    texto = getFile(file)\n",
    "    for i in range(0, n):\n",
    "        textoTokenizado = sent_tokenize(texto)\n",
    "        clean1 = clean2(textoTokenizado)\n",
    "        txtClean = clean(clean1)\n",
    "        TFIDF_scores,txtDic = frequency(txtClean)\n",
    "        TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 7)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, clean1, txtDic)\n",
    "        #print(sentence_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        print(summary)\n",
    "        #print(len(texto))\n",
    "        #print(len(summary))\n",
    "        texto = summary\n",
    "resumir('texto.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a string with special characters \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"This is a string with special characters: !@#$%^&*()_+`-=[]{};:'\\\\|,.<>/?\"\n",
    "# Define a regular expression pattern that matches special characters\n",
    "pattern = r\"[^a-zA-Z0-9 ]\"\n",
    "result = re.sub(pattern, \"\", text)\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
