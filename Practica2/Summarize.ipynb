{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import operator\n",
    "import statistics\n",
    "import math \n",
    "import re\n",
    "\n",
    "stop_words = set(stopwords.words('spanish') + list(punctuation))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def getFile(fname):\n",
    "    try:\n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "def clean2(textoTokenizado):\n",
    "    newTokens = []\n",
    "    for token in textoTokenizado:\n",
    "        pattern = r\"[^a-zA-Z0-9 \\n.,()¡!¿?áéóíúÁÉÍÓÚ]\"\n",
    "\n",
    "        stripped = re.sub(pattern, \"\", token)\n",
    "\n",
    "        # Change any whitespace to one space\n",
    "        stripped = ' '.join(stripped.split())\n",
    "\n",
    "        # Remove start and end white spaces\n",
    "        stripped = stripped.strip()\n",
    "        \n",
    "        newTokens.append(stripped)\n",
    "    return newTokens\n",
    "\n",
    "def clean(textoTokenizado):\n",
    "    newTokens = []\n",
    "    for token in textoTokenizado:\n",
    "        pattern = r\"[^a-zA-Z0-9 \\n]\"\n",
    "\n",
    "        stripped = re.sub(pattern, \"\", token)\n",
    "\n",
    "        # Change any whitespace to one space\n",
    "        stripped = ' '.join(stripped.split())\n",
    "\n",
    "        # Remove start and end white spaces\n",
    "        stripped = stripped.strip()\n",
    "        \n",
    "        newTokens.append(stripped)\n",
    "    return newTokens\n",
    "\n",
    "def global_frequency(txtClean):\n",
    "\n",
    "    freq_table = {}\n",
    "    text = ' '.join(txtClean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "    return freq_table\n",
    "\n",
    "\n",
    "def get_keywords(txtClean, n):\n",
    "\n",
    "    freq_table = global_frequency(txtClean)\n",
    "    #sort in descending order\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = False) \n",
    "    #print(freq_table_sorted)\n",
    "    keywords = []\n",
    "    for i in range(0, n):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords\n",
    "\n",
    "def weigh_keywords(txtClean, TFIDF_scores, n):\n",
    "\n",
    "    keywords = get_keywords(txtClean, n)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['word'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores\n",
    "\n",
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "\n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['sentence_id'] == temp_dict['sentence_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'sentence_id' : doc['sentence_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['sentence_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "    return sentence_info\n",
    "\n",
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "\n",
    "        if(sent['sent_score']) >= avg: # + 1.5*stdev:\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(sentences):\n",
    "    TFIDF_scores = []\n",
    "    num_words_per_sentence = []\n",
    "    doc_info = []\n",
    "    datos = []\n",
    "\n",
    "    for i, sent in enumerate(sentences, start=1):\n",
    "        words = sent.split()\n",
    "        count = len(words)\n",
    "        temp = {'sentence_id': i, 'num_words': count, 'wordsfreq': {}}\n",
    "\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word not in stop_words:\n",
    "                if word in temp['wordsfreq']:\n",
    "                    temp['wordsfreq'][word] += 1\n",
    "                else:\n",
    "                    temp['wordsfreq'][word] = 1\n",
    "\n",
    "        for word, freq in temp['wordsfreq'].items():\n",
    "            temp['wordsfreq'][word] = freq / count\n",
    "        temp2 = {'sentece_id':i, 'word':word}\n",
    "        doc_info.append(temp)\n",
    "    \n",
    "        num_words_per_sentence.append({'sentence_id': i, 'num_words': count})\n",
    "        \n",
    "    IDF_scores = []\n",
    "    for word in set(word for doc in doc_info for word in doc['wordsfreq']):\n",
    "        count = sum(1 for doc in doc_info if word in doc['wordsfreq'])\n",
    "        idf_score = math.log(len(doc_info) / count)\n",
    "        IDF_scores.append({'word': word, 'IDF_score': idf_score})\n",
    "        #print(IDF_scores)\n",
    "    # Calcular la puntuación TF-IDF para cada palabra en cada oración y almacenar en TFIDF_scores\n",
    "    for doc in doc_info:\n",
    "        for word, tf_score in doc['wordsfreq'].items():\n",
    "            idf_score = next(item['IDF_score'] for item in IDF_scores if item['word'] == word)\n",
    "            tfidf_score = tf_score * idf_score\n",
    "            TFIDF_scores.append({'sentence_id': doc['sentence_id'], 'word': word, 'TFIDF_score': tfidf_score})\n",
    "        \n",
    "\n",
    "    return TFIDF_scores, num_words_per_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumir(file, n):\n",
    "    texto = getFile(file)\n",
    "    for i in range(0, n):\n",
    "        textoTokenizado = sent_tokenize(texto)\n",
    "        clean1 = clean2(textoTokenizado)\n",
    "        txtClean = clean(clean1)\n",
    "        TFIDF_scores,txtDic = frequency(txtClean)\n",
    "        TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 7)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, clean1, txtDic)\n",
    "        #print(sentence_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        print(summary)\n",
    "        print(len(texto))\n",
    "        print(len(summary))\n",
    "        texto = summary\n",
    "resumir('texto.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume(file, n):\n",
    "    texto = getFile(file)\n",
    "    for i in range(0, n):\n",
    "        textoTokenizado = sent_tokenize(texto)\n",
    "        clean1 = clean2(textoTokenizado)\n",
    "        txtClean = clean(clean1)\n",
    "        TFIDF_scores,txtDic = frequency(txtClean)\n",
    "        TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 7)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, clean1, txtDic)\n",
    "        #print(sentence_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        #print(summary)\n",
    "        print(len(texto))\n",
    "        print(len(summary))\n",
    "        texto = summary\n",
    "    return texto\n",
    "#resumir('texto.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resumeF(texto, n):\n",
    "    for i in range(0, n):\n",
    "        textoTokenizado = sent_tokenize(texto)\n",
    "        clean1 = clean2(textoTokenizado)\n",
    "        txtClean = clean(clean1)\n",
    "        TFIDF_scores,txtDic = frequency(txtClean)\n",
    "        TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 7)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, clean1, txtDic)\n",
    "        #print(sentence_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        print(summary)\n",
    "        print(len(texto))\n",
    "        print(len(summary))\n",
    "        texto = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9895\n",
      "4267\n",
      "5062\n",
      "1547\n",
      "5680\n",
      "3282\n",
      "5662\n",
      "1640\n",
      "5330\n",
      "2725\n",
      "El futuro de la IA hacia inteligencias artificiales realmente inteligentes Ramón López de Mántaras Instituto de Investigación en Inteligencia Artificial (IIIA), Bellaterra, Espaa Este capítulo contiene algunas reflexiones sobre inteligencia artificial (IA).\n",
      "Por ejemplo, en el siglo XVII, Descartes se preguntó si un complejo sistema mecánico compuesto de engranajes, poleas y tubos podría, en principio, emular el pensamiento.\n",
      "Un SSF consiste en un conjunto de entidades denominadas símbolos que, mediante relaciones, pueden ser combinados formando estructuras más grandes como los átomos que se combinan formando moléculas y que pueden ser transformados aplicando un conjunto de procesos.\n",
      "Estos procesos pueden generar nuevos símbolos, crear y modificar relaciones entre símbolos, almacenar símbolos, comparar si dos símbolos son iguales o distintos, etcétera.\n",
      "En definitiva, de acuerdo con la hipótesis SSF, la naturaleza del sustrato (circuitos electrónicos o redes neuronales) carece de importancia siempre y cuando dicho sustrato permita procesar símbolos.\n",
      "En cualquier caso, su validez o refutación se deberá verificar de acuerdo con el método científico, con ensayos experimentales.\n",
      "Exhibir inteligencia específica es otra cosa bien distinta.\n",
      "De hecho, sigue siendo muy importante y actualmente se considera el modelo clásico en IA (también denominado por el acrónimo GOFAI, de Good Old Fashioned AI).\n",
      "Es decir, la IA simbólica opera con representaciones abstractas del mundo real que se modelan mediante lenguajes de representación basados principalmente en la lógica matemática y sus extensiones.\n",
      "Sin embargo, hay ciertas diferencias.\n",
      "La ciencia de datos combina estadísticas, informática y conocimiento empresarial para extraer valor de distintos orígenes de datos.\n",
      "Esta capacidad puede generar importantes ventajas empresariales.\n",
      "La IA posee valor para casi todas las funciones, negocios e industrias.\n",
      "Máquinas reactivas IA limitada que solo reacciona a diferentes tipos de estímulos basados en reglas preprogramadas.\n",
      "El agente recibe un refuerzo positivo cuando realiza la tarea de forma correcta y un refuerzo negativo cuando tiene bajo rendimiento.\n",
      "Un médico podría aprovechar la visión artificial para identificar anomalías en imágenes médicas y realizar diagnósticos más precisos.\n",
      "3.Desarrollar habilidades tecnológicas básicas Tener conocimientos técnicosinformáticos básicos permite comprender mejor cómo funciona la IA, mejora la capacidad de resolución de problemas y acelera la adaptación a las últimas tecnologías, además de permitir una evaluación crítica de las soluciones basadas en la IA.\n",
      "Esto incluye familiarizarse con dispositivos, sistemas operativos y conceptos informáticos esenciales.\n",
      "No solo necesita conocimientos teóricos, sino a través de la práctica continua.\n",
      "5.Aprender con regularidad La IA avanza rápidamente, con nuevas técnicas, algoritmos y aplicaciones emergiendo constantemente.\n",
      "El crecimiento de habilidades ha crecido de la siguiente manera ChatGPT (4.419), Azure Machine Learning (281), AI Art Generation (239), Amazon EMR (227) y Midjourney (218).\n",
      "Definitivamente con esta nueva estrategia queremos traer más innovación al mercado lo antes posible, resaltó en el marco del Lenovo Tech World 2023.\n",
      "Wong aadió que Care of One Platform estará disponible en unas semanas, aunque todavía no definen en qué geografías se lanzará primero.\n",
      "Marco Jiménez, director general de Lenovo México, confirmó la presencia de clientes nacionales como es el caso de Kavak, una startup de autos mexicana que está presente en casi toda América Latina, en Espaa, Turquía y está abriendo Dubái.\n",
      "Entonces están preocupados por cómo llevar la inteligencia artificial a sus procesos para mejorar la experiencia del cliente, para mejorar cómo organizan los autos, cómo los almacenan, cuál tiene que ser el tiempo de reposición están viendo muchas posibilidades de implementación de diferentes soluciones, relató.\n",
      "Marco Jiménez, director general de Lenovo México, recordó que la crisis sanitaria provocó que el mercado de cómputo creciera cerca de 20 puntos y pasara de 4 millones a 5 millones de unidades en el país en tan sólo dos aos.\n",
      "Hoy día estamos regresando a los 4 millones, estamos regresando a los niveles prepandemia y Lenovo en esta etapa vamos ganando participación de mercado, aseguró en el marco de Lenovo Tech World 2023.\n",
      "13465\n",
      "4359\n"
     ]
    }
   ],
   "source": [
    "def main (text1, text2, text3, text4, text5):\n",
    "    summary1 = resume (text1,1)\n",
    "    summary2 = resume (text2,1)\n",
    "    summary3 = resume (text3,1)\n",
    "    summary4 = resume (text4,1)\n",
    "    summary5 = resume (text5,1)\n",
    "    textos = [summary1, summary2, summary3, summary4, summary5]\n",
    "    resultado = \" \". join(textos)\n",
    "    resumeF(resultado,1)\n",
    "\n",
    "\n",
    "\n",
    "texto1 = 'texto.txt'\n",
    "texto2 = 'texto1.txt'\n",
    "texto3 = 'texto2.txt'\n",
    "texto4 = 'texto3.txt'\n",
    "texto5 = 'texto4.txt'\n",
    "\n",
    "main(texto1,texto2,texto3,texto4,texto5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
