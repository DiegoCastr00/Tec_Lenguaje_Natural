{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import operator\n",
    "import statistics\n",
    "stop_words = set(stopwords.words('spanish') + list(punctuation))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def getFile(fname):\n",
    "    try:\n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "def clean(txt):\n",
    "    textoTokenizado = sent_tokenize(txt)\n",
    "    newTokens = []\n",
    "    for token in textoTokenizado:\n",
    "        special_chars = \"!@#$%^&*()-_=+[]{}|;:'\\\",.<>/?`~\"\n",
    "        \n",
    "        # Replace special characters with ' '\n",
    "        stripped = ''.join(char if char.isalnum() or char.isspace() or char in special_chars else ' ' for char in token)\n",
    "\n",
    "        # Change any whitespace to one space\n",
    "        stripped = ' '.join(stripped.split())\n",
    "\n",
    "        # Remove start and end white spaces\n",
    "        stripped = stripped.strip()\n",
    "        \n",
    "        newTokens.append(stripped)\n",
    "    return newTokens\n",
    "\n",
    "#unir conteo palabras,, freq palabras y tf oracion \n",
    "\n",
    "def countWordSentence(sentences):\n",
    "    # Se crea un diccionario guardando el numero de palabras por oracion\n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in sentences:\n",
    "        i += 1 \n",
    "        words = sent.split()\n",
    "        count = len(words)\n",
    "        temp = {'sentence_id' : i, 'num_words' : count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info\n",
    "\n",
    "def create_freq_dict(sents):\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word not in stop_words:\n",
    "                if word in freq_dict:\n",
    "                    freq_dict[word] += 1\n",
    "                else:\n",
    "                    freq_dict[word] = 1\n",
    "                temp = {'sentence_id' : i, 'wordsfreq': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list\n",
    "\n",
    "\n",
    "def computeTF(doc_info, freqDict_list):\n",
    "    TF_scores = []\n",
    "    \n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['sentence_id']\n",
    "        for k in tempDict['wordsfreq']:\n",
    "            temp = {'sentence_id' : id,\n",
    "                    'TF_score' : tempDict['wordsfreq'][k]/doc_info[id-1]['num_words'],\n",
    "                     'word': k}\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores\n",
    "\n",
    "\n",
    "import math \n",
    "def computeIDF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    idf = ln(total number of docs/number of docs with term in it)\n",
    "    \"\"\"\n",
    "    \n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['wordsfreq'].keys():\n",
    "            count = sum([k in tempDict['wordsfreq'] for tempDict in freqDict_list])\n",
    "            temp = {'sentence_id' : counter, 'IDF_score' : math.log(len(doc_info)/count), 'word' : k}\n",
    "    \n",
    "            IDF_scores.append(temp)\n",
    "                \n",
    "    return IDF_scores\n",
    "\n",
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    \"\"\"\n",
    "    TFIDF is computed by multiplying the coressponding\n",
    "    TF and IDF values of each term. \n",
    "    \"\"\"\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['word'] == i['word'] and j['sentence_id'] == i['sentence_id']:\n",
    "                temp = {'sentence_id' : i['sentence_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                       'word' : i['word']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores\n",
    "\n",
    "#se une hasta aqui\n",
    "\n",
    "def global_frequency(txtClean):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary with the frequency \n",
    "    count of every word in the text\n",
    "    \"\"\"\n",
    "    freq_table = {}\n",
    "    text = ' '.join(txtClean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        word = ps.stem(word)\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "                \n",
    "    return freq_table\n",
    "\n",
    "\n",
    "def get_keywords(txtClean, n):\n",
    "    \"\"\"\n",
    "    This function gets the top 5 most\n",
    "    frequently occuring words in the whole text\n",
    "    and stores them as keywords\n",
    "    \"\"\"\n",
    "    freq_table = global_frequency(txtClean)\n",
    "    #sort in descending order\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = False) \n",
    "    #print(freq_table_sorted)\n",
    "    keywords = []\n",
    "    for i in range(0, n):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords\n",
    "\n",
    "def weigh_keywords(txtClean, TFIDF_scores, n):\n",
    "    \"\"\"\n",
    "    This function doubles the TFIDF score\n",
    "    of the words that are keywords\n",
    "    \"\"\"\n",
    "    keywords = get_keywords(txtClean, n)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['word'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores\n",
    "\n",
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "    \"\"\"\n",
    "    This function prints out the summary and returns the \n",
    "    score of each sentence in a list.\n",
    "    \n",
    "    The score of a sentence is calculated by adding the TFIDF\n",
    "    scores of the words that make up the sentence.\n",
    "    \"\"\"\n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "        \"\"\"\n",
    "        This loops through each document(sentence)\n",
    "        and calculates their 'sent_score'\n",
    "        \"\"\"\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['sentence_id'] == temp_dict['sentence_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'sentence_id' : doc['sentence_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['sentence_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "    return sentence_info\n",
    "\n",
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sum of scores\n",
    "        of all the sentences.\n",
    "        \"\"\"\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sentence scores \n",
    "        and stores them in an array.\n",
    "        \"\"\"\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop is for getting the sumamry by \n",
    "        extracting sentences by an if clause\n",
    "        \"\"\"\n",
    "        if(sent['sent_score']) >= avg: # + 1.5*stdev:\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'countWordSentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Repositorios\\Tec_Lenguaje_Natural\\Practica2\\Summarize.ipynb Celda 2\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         texto \u001b[39m=\u001b[39m summary\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m resumir(\u001b[39m'\u001b[39;49m\u001b[39mtexto.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32md:\\Repositorios\\Tec_Lenguaje_Natural\\Practica2\\Summarize.ipynb Celda 2\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, n):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     txtClean \u001b[39m=\u001b[39m clean(texto)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     txtDic \u001b[39m=\u001b[39m countWordSentence(txtClean)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     freqDict \u001b[39m=\u001b[39m create_freq_dict(txtClean)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Repositorios/Tec_Lenguaje_Natural/Practica2/Summarize.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     TF \u001b[39m=\u001b[39m computeTF(txtDic,freqDict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'countWordSentence' is not defined"
     ]
    }
   ],
   "source": [
    "def resumir(file, n):\n",
    "    texto = getFile(file)\n",
    "    for i in range(0, n):\n",
    "        txtClean = clean(texto)\n",
    "        txtDic = countWordSentence(txtClean)\n",
    "        freqDict = create_freq_dict(txtClean)\n",
    "        TF = computeTF(txtDic,freqDict)\n",
    "        IDF = computeIDF(txtDic,freqDict)\n",
    "        TFIDF_scores = computeTFIDF(TF, IDF)\n",
    "        print(TFIDF_scores)\n",
    "        TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 7)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, txtClean, txtDic)\n",
    "        print(sentence_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        print(summary)\n",
    "        print(len(texto))\n",
    "        print(len(summary))\n",
    "        texto = summary\n",
    "    return \n",
    "resumir('texto.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Palabra  Frecuencia1  Frecuencia2  Frecuencia3  Frecuencia4\n",
      "0         futuro            1      0.03125     3.912023     0.244501\n",
      "1             ia            2      0.06250     3.892605     0.243288\n",
      "2          hacia            1      0.03125     3.901973     0.121937\n",
      "3   inteligencia            3      0.09375     3.894085     0.365070\n",
      "4     artificial            1      0.03125     3.901973     0.121937\n",
      "5       realment            1      0.03125     3.912023     0.244501\n",
      "6     inteligent            1      0.03125     3.893859     0.121683\n",
      "7           ramn            1      0.03125     3.912023     0.244501\n",
      "8           lpez            1      0.03125     3.912023     0.244501\n",
      "9         mntara            1      0.03125     3.912023     0.244501\n",
      "10     instituto            1      0.03125     3.912023     0.244501\n",
      "11  investigacin            1      0.03125     3.912023     0.244501\n",
      "12      artifici            2      0.06250     3.901973     0.243873\n",
      "13          iiia            1      0.03125     3.912023     0.122251\n",
      "14    bellaterra            1      0.03125     3.912023     0.122251\n",
      "15         espaa            1      0.03125     3.912023     0.122251\n",
      "16           est            1      0.03125     3.894368     0.121699\n",
      "17       captulo            1      0.03125     3.912023     0.122251\n",
      "18       contien            1      0.03125     3.912023     0.122251\n",
      "19        alguna            1      0.03125     3.912023     0.122251\n",
      "20     reflexion            1      0.03125     3.912023     0.122251\n",
      "21          sobr            1      0.03125     3.895894     0.121747\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def resumir(file, n):\n",
    "    texto = getFile(file)\n",
    "    for i in range(0, n):\n",
    "        txtClean = clean(texto)\n",
    "        txtDic = countWordSentence(txtClean)\n",
    "        freqDict = create_freq_dict(txtClean)\n",
    "        TF = computeTF(txtDic,freqDict)\n",
    "        IDF = computeIDF(txtDic,freqDict)\n",
    "        TFIDF_scores = computeTFIDF(TF, IDF)\n",
    "        print(TFIDF_scores)\n",
    "        TFIDF_scores = weigh_keywords(txtClean, TFIDF_scores, 7)\n",
    "        sentence_info = get_sent_score(TFIDF_scores, txtClean, txtDic)\n",
    "        print(sentence_info)\n",
    "        summary = get_summary(sentence_info)\n",
    "        print(summary)\n",
    "        print(len(texto))\n",
    "        print(len(summary))\n",
    "        texto = summary\n",
    "    return \n",
    "resumir('texto.txt', 1)\n",
    "\n",
    "# Diccionarios\n",
    "diccionario1 = {'futuro': 1, 'ia': 2, 'hacia': 1, 'inteligencia': 3, 'artificial': 1, 'realment': 1, 'inteligent': 1, 'ramn': 1, 'lpez': 1, 'mntara': 1, 'instituto': 1, 'investigacin': 1, 'artifici': 2, 'iiia': 1, 'bellaterra': 1, 'espaa': 1, 'est': 1, 'captulo': 1, 'contien': 1, 'alguna': 1, 'reflexion': 1, 'sobr': 1}\n",
    "diccionario2 = {'futuro': 0.03125, 'ia': 0.0625, 'hacia': 0.03125, 'inteligencia': 0.09375, 'artificial': 0.03125, 'realment': 0.03125, 'inteligent': 0.03125, 'ramn': 0.03125, 'lpez': 0.03125, 'mntara': 0.03125, 'instituto': 0.03125, 'investigacin': 0.03125, 'artifici': 0.0625, 'iiia': 0.03125, 'bellaterra': 0.03125, 'espaa': 0.03125, 'est': 0.03125, 'captulo': 0.03125, 'contien': 0.03125, 'alguna': 0.03125, 'reflexion': 0.03125, 'sobr': 0.03125}\n",
    "diccionario3 = {'futuro': 3.912023005428146, 'ia': 3.8926049195710446, 'hacia': 3.901972669574645, 'inteligencia': 3.8940853047414787, 'artificial': 3.901972669574645, 'realment': 3.912023005428146, 'inteligent': 3.893859034800475, 'ramn': 3.912023005428146, 'lpez': 3.912023005428146, 'mntara': 3.912023005428146, 'instituto': 3.912023005428146, 'investigacin': 3.912023005428146, 'artifici': 3.901972669574645, 'iiia': 3.912023005428146, 'bellaterra': 3.912023005428146, 'espaa': 3.912023005428146, 'est': 3.8943680701894254, 'captulo': 3.912023005428146, 'contien': 3.912023005428146, 'alguna': 3.912023005428146, 'reflexion': 3.912023005428146, 'sobr': 3.8958936234982624}\n",
    "diccionario4 = {'futuro': 0.24450143783925912, 'ia': 0.2432878074731903, 'hacia': 0.12193664592420765, 'inteligencia': 0.36507049731951363, 'artificial': 0.12193664592420765, 'realment': 0.24450143783925912, 'inteligent': 0.12168309483751484, 'ramn': 0.24450143783925912, 'lpez': 0.24450143783925912, 'mntara': 0.24450143783925912, 'instituto': 0.24450143783925912, 'investigacin': 0.24450143783925912, 'artifici': 0.2438732918484153, 'iiia': 0.12225071891962956, 'bellaterra': 0.12225071891962956, 'espaa': 0.12225071891962956, 'est': 0.12169900219341954, 'captulo': 0.12225071891962956, 'contien': 0.12225071891962956, 'alguna': 0.12225071891962956, 'reflexion': 0.12225071891962956, 'sobr': 0.1217466757343207}\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
