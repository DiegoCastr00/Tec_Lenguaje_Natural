{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import math\n",
    "import operator\n",
    "import statistics\n",
    "from string import punctuation\n",
    "stop_words = set(stopwords.words('spanish') + list(punctuation))\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFile(fname):\n",
    "    try:\n",
    "        with open(fname, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_five_txt_files(file_paths):\n",
    "  \"\"\"Reads the contents of five text files and returns a list of strings.\n",
    "  Args:\n",
    "    file_paths: A list of five file paths.\n",
    "  Returns:\n",
    "    A list of strings, where each string is the contents of one of the text files.\n",
    "  \"\"\"\n",
    "  texts = []\n",
    "  for file_path in file_paths:\n",
    "    with open(file_path, \"r\") as f:\n",
    "      texts.append(f.read())\n",
    "  return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_string_special_characters(s, convert_to_lower=True):\n",
    "\n",
    "    # Replace special character with ' '\n",
    "    stripped = re.sub('[^\\w\\s]', '', s)\n",
    "    stripped = re.sub('_', '', stripped)\n",
    "\n",
    "    # Change any whitespace to one space\n",
    "    stripped = re.sub('\\s+', ' ', stripped)\n",
    "\n",
    "    # Remove start and end white spaces\n",
    "    stripped = stripped.strip()\n",
    "\n",
    "    # Convert to lowercase if specified\n",
    "    if convert_to_lower:\n",
    "        stripped = stripped.lower()\n",
    "    \n",
    "    return stripped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    \"\"\"This function returns the \n",
    "    total number of words in the input text.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc(text_sents_clean):\n",
    "    \n",
    "    doc_info = []\n",
    "    i = 0\n",
    "    for sent in text_sents_clean:\n",
    "        i += 1 \n",
    "        count = count_words(sent)\n",
    "        temp = {'doc_id' : i, 'doc_length' : count}\n",
    "        doc_info.append(temp)\n",
    "    return doc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freq_dict(sents):\n",
    "    \"\"\"\n",
    "    This function creates a frequency dictionary\n",
    "    of each document that contains words other than\n",
    "    stop words.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    freqDict_list = []\n",
    "    for sent in sents:\n",
    "        i += 1\n",
    "        freq_dict = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            #word = word.lower()\n",
    "            if word not in stop_words:\n",
    "                if word in freq_dict:\n",
    "                    freq_dict[word] += 1\n",
    "                else:\n",
    "                    freq_dict[word] = 1\n",
    "                temp = {'doc_id' : i, 'freq_dict': freq_dict}\n",
    "        freqDict_list.append(temp)\n",
    "    return freqDict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_frequency(text_sents_clean):\n",
    "    \"\"\"\n",
    "    This function returns a dictionary with the frequency \n",
    "    count of every word in the text\n",
    "    \"\"\"\n",
    "    freq_table = {}\n",
    "    text = ' '.join(text_sents_clean) #join the cleaned sentences to get the text \n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text_sents_clean):\n",
    "\n",
    "    freq_table = global_frequency(text_sents_clean)\n",
    "    #sort in descending order\n",
    "    freq_table_sorted = sorted(freq_table.items(), key = operator.itemgetter(1), reverse = True) \n",
    "    keywords = []\n",
    "    for i in range(0, 5):  #taking first 5 most frequent words\n",
    "        keywords.append(freq_table_sorted[i][0])\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    tf = (frequency of the term in the doc/total number of terms in the doc)\n",
    "    \"\"\"\n",
    "    TF_scores = []\n",
    "    \n",
    "    for tempDict in freqDict_list:\n",
    "        id = tempDict['doc_id']\n",
    "        for k in tempDict['freq_dict']:\n",
    "            temp = {'doc_id' : id,\n",
    "                    'TF_score' : tempDict['freq_dict'][k]/doc_info[id-1]['doc_length'],\n",
    "                   'key' : k}\n",
    "            TF_scores.append(temp)\n",
    "    return TF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(doc_info, freqDict_list):\n",
    "    \"\"\"\n",
    "    idf = ln(total number of docs/number of docs with term in it)\n",
    "    \"\"\"\n",
    "    \n",
    "    IDF_scores = []\n",
    "    counter = 0\n",
    "    for dict in freqDict_list:\n",
    "        counter += 1\n",
    "        for k in dict['freq_dict'].keys():\n",
    "            count = sum([k in tempDict['freq_dict'] for tempDict in freqDict_list])\n",
    "            temp = {'doc_id' : counter, 'IDF_score' : math.log(len(doc_info)/count), 'key' : k}\n",
    "    \n",
    "            IDF_scores.append(temp)\n",
    "                \n",
    "    return IDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(TF_scores, IDF_scores):\n",
    "    \"\"\"\n",
    "    TFIDF is computed by multiplying the coressponding\n",
    "    TF and IDF values of each term. \n",
    "    \"\"\"\n",
    "    TFIDF_scores = []\n",
    "    for j in IDF_scores:\n",
    "        for i in TF_scores:\n",
    "            if j['key'] == i['key'] and j['doc_id'] == i['doc_id']:\n",
    "                temp = {'doc_id' : i['doc_id'],\n",
    "                        'TFIDF_score' : j['IDF_score']*i['TF_score'],\n",
    "                       'key' : i['key']}\n",
    "        TFIDF_scores.append(temp)\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weigh_keywords(TFIDF_scores,text_sents_clean):\n",
    "\n",
    "    keywords = get_keywords(text_sents_clean)\n",
    "    for temp_dict in TFIDF_scores:\n",
    "        if temp_dict['key'] in keywords:\n",
    "            temp_dict['TFIDF_score'] *= 2\n",
    "    return TFIDF_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizador(texto):\n",
    "    oraciones= ['. ','\\n']\n",
    "    tokens = []\n",
    "\n",
    "    token_actual = ''\n",
    "    for caracter in texto:\n",
    "        #if caracter == ' ' or caracter in caracteres:\n",
    "        if caracter in oraciones:\n",
    "            if token_actual:\n",
    "                tokens.append(token_actual)\n",
    "                token_actual = ''\n",
    "        else:\n",
    "            token_actual += caracter\n",
    "\n",
    "    if token_actual:\n",
    "        tokens.append(token_actual)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_score(TFIDF_scores, text_sents, doc_info):\n",
    "\n",
    "    sentence_info = []\n",
    "    for doc in doc_info:\n",
    "\n",
    "        sent_score = 0\n",
    "        for i in range(0, len(TFIDF_scores)):\n",
    "            temp_dict = TFIDF_scores[i]\n",
    "            if doc['doc_id'] == temp_dict['doc_id']:\n",
    "                sent_score += temp_dict['TFIDF_score']\n",
    "        temp = {'doc_id' : doc['doc_id'], 'sent_score' : sent_score,\n",
    "                'sentence' : text_sents[doc['doc_id']-1]}\n",
    "        sentence_info.append(temp)\n",
    "\n",
    "    return sentence_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_puntuacion_y_stopwords(texto):\n",
    "    # Eliminar signos de puntuación\n",
    "    texto_sin_puntuacion = \"\".join([caracter for caracter in texto if caracter not in string.punctuation])\n",
    "    # Tokenizar el texto\n",
    "    tokens = texto_sin_puntuacion.split()\n",
    "\n",
    "    \n",
    "    # Eliminar stopwords\n",
    "    stopwords_spanish = set(stopwords.words('spanish'))\n",
    "    tokens_sin_stopwords = [palabra for palabra in tokens if palabra.lower() not in stopwords_spanish]\n",
    "\n",
    "    # Unir las palabras nuevamente en una cadena de texto\n",
    "    texto_procesado = \" \".join(tokens_sin_stopwords)\n",
    "\n",
    "    return texto_procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(sentence_info):\n",
    "    sum = 0\n",
    "    summary = []\n",
    "    array = []\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sum of scores\n",
    "        of all the sentences.\n",
    "        \"\"\"\n",
    "        sum += temp_dict['sent_score']\n",
    "    avg = sum/len(sentence_info) #computing the average tf-idf score\n",
    "    for temp_dict in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop gets the sentence scores \n",
    "        and stores them in an array.\n",
    "        \"\"\"\n",
    "        array.append(temp_dict['sent_score'])\n",
    "    stdev = statistics.stdev(array) #computing standard deviation on the array   \n",
    "    for sent in sentence_info:\n",
    "        \"\"\"\n",
    "        This loop is for getting the sumamry by \n",
    "        extracting sentences by an if clause\n",
    "        \"\"\"\n",
    "        if(sent['sent_score']) >= avg: # + 1.5*stdev:\n",
    "            summary.append(sent['sentence'])\n",
    "    summary = '\\n'.join(summary)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inteligencia artificial general (AGI)\n",
      "Esta categoría -Artificial General Intelligence- se alcanza cuando una máquina adquiere capacidades cognitivas a nivel humano.\n",
      "Es decir, cuando puede realizar cualquier tarea intelectual que realiza una persona.\n",
      "Carlos Ignacio Gutiérrez, investigador de políticas públicas en el Future of Life Institute, explicó a BBC Mundo que uno de los grandes desafíos que presenta la IA es que \"no existe un cuerpo colegiado de expertos que deciden cómo regularlo, como ocurre, por ejemplo, con el Panel Intergubernamental sobre Cambio Climático (IPCC)\".\n",
      "\"¿Deberíamos desarrollar mentes no humanas que eventualmente podrían superarnos en número, ser más inteligentes, hacernos obsoletos y reemplazarnos?\", cuestionaron.\n"
     ]
    }
   ],
   "source": [
    "with open('prueba2_ia.txt', 'r', encoding='utf-8') as file:\n",
    "    texto = file.read()\n",
    "oraciones = tokenizador(texto)\n",
    "#oraciones = sent_tokenize(text)\n",
    "\n",
    "oraciones_lim = [remove_string_special_characters(s) for s in oraciones]#Se va ingresando por oración \n",
    "doc_info = get_doc(oraciones_lim) # Se encarga  de odtener la longitud de cada uno de las oraciones eliminando las Stopwords\n",
    "\n",
    "freqDict_list = create_freq_dict(oraciones_lim)\n",
    "TF_scores = computeTF(doc_info, freqDict_list)\n",
    "IDF_scores = computeIDF(doc_info, freqDict_list)\n",
    "\n",
    "\n",
    "TFIDF_scores = computeTFIDF(TF_scores, IDF_scores)\n",
    "TFIDF_scores = weigh_keywords(TFIDF_scores,oraciones_lim)\n",
    "sentence_info = get_sent_score(TFIDF_scores, oraciones, doc_info)\n",
    "\n",
    "doc_info = get_doc(oraciones_lim)\n",
    "summary = get_summary(sentence_info)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
